<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>树华的博客</title>
  
  <subtitle>一个简单的故事</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.willshuhua.me/"/>
  <updated>2018-01-16T08:31:32.717Z</updated>
  <id>http://blog.willshuhua.me/</id>
  
  <author>
    <name>树华</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>scrapy专利爬虫（二）——请求相关</title>
    <link href="http://blog.willshuhua.me/2018/01/16/scrapy%E4%B8%93%E5%88%A9%E7%88%AC%E8%99%AB%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%E8%AF%B7%E6%B1%82%E7%9B%B8%E5%85%B3/"/>
    <id>http://blog.willshuhua.me/2018/01/16/scrapy专利爬虫（二）——请求相关/</id>
    <published>2018-01-16T08:30:26.049Z</published>
    <updated>2018-01-16T08:31:32.717Z</updated>
    
    <content type="html"><![CDATA[<p>在这里笔者将会介绍一些关于发送request的相关内容。</p><h2 id="Spider"><a href="#Spider" class="headerlink" title="Spider"></a>Spider</h2><p>Spider默认需要填写三个参数：</p><table><br><tr><br><td>name</td><br><td>spider的独立名称，必须唯一</td><br></tr><br><tr><br><td>allowed_domains</td><br><td>允许爬取的范围，以专利爬虫为例，不会超出专利网站的范围，所以只需要填写”pss-system.gov.cn”即可。</td><br></tr><br><tr><br><td>start_urls</td><br><td>起始url，spider会首先请求这个参数里的地址。</td><br></tr><br><tr><br></tr></table><h2 id="Request和FormRequest"><a href="#Request和FormRequest" class="headerlink" title="Request和FormRequest"></a>Request和FormRequest</h2><p>在Spider之后，如果想继续深入爬取，可以使用Requset或FormRequest对象建立新的链接。</p><ul><li>需要指定Request/FormRequset的callback参数，这是一个含reponse的回调函数，这样当请求完成后，scrapy会自动调用所设定的回调函数处理reponse。</li><li>参数传递，Request和FormRequest都用meta这个参数，这个参数会跟随reponse在callback参数的回调函数中出现。所以可以进行参数传递。</li><li>设定请求的method是get，post等。</li><li>关于FormRequest，这是一个可以用来发送表单数据的请求，有个formData参数，可带tuple格式参数</li><li>其他参数详见<a href="http://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/request-response.html#request-objects" target="_blank" rel="noopener">官方文档</a></li></ul><p>在这里必须要解释的一个python关键字是<code>yield</code>，笔者的理解这是一个迭代对象生成器。对于list等可迭代对象，可以用函数的方式生成，只要在函数中使用yield这个关键字，代码在执行的时候会先挂起yield前的所有操作，当对象产生迭代行为，如for循环时才会执行前面的代码。在scrapy中将会经常使用这个关键字，发送请求。</p><h2 id="下载延时"><a href="#下载延时" class="headerlink" title="下载延时"></a>下载延时</h2><p>在settings.py文件中，可以设置请求完成后的文件下载延时DOWNLOAD_DELAY，为了减轻服务器的压力这个延时不能设置得太低。</p><h2 id="部分中间件"><a href="#部分中间件" class="headerlink" title="部分中间件"></a>部分中间件</h2><ul><li>UserAgentMiddleware</li></ul><p>可以用来随机替换程序发送请求请求的User-Agent参数，伪装各种浏览器，不会轻易被服务器发现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomUserAgentMiddleware</span><span class="params">(UserAgentMiddleware)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        request.headers.setdefault(<span class="string">'User-Agent'</span>, HeadersEngine().getRandomUserAgent())</span><br></pre></td></tr></table></figure><ul><li>HttpProxyMiddleware</li></ul><p>可以用来实现ip代理的功能</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProxyMiddleware</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        request.meta[<span class="string">'proxy'</span>] = <span class="string">"http://"</span> + 你的代理ip + : + 你的代理ip的端口号</span><br></pre></td></tr></table></figure><ul><li>如何开启中间件</li></ul><p>在settings.py文件中，有这样一段代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">'PatentCrawler.middlewares.ProxyMiddleware'</span>: <span class="number">543</span>,</span><br><span class="line">    <span class="string">'PatentCrawler.middlewares.RandomUserAgentMiddleware'</span>: <span class="number">542</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>需要在花括号内将定义的中间件填写上去，并填写优先级。</p><h2 id="源码下载"><a href="#源码下载" class="headerlink" title="源码下载"></a>源码下载</h2><ul><li><a href="https://github.com/will4906/PatentCrawler" target="_blank" rel="noopener">github</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在这里笔者将会介绍一些关于发送request的相关内容。&lt;/p&gt;
&lt;h2 id=&quot;Spider&quot;&gt;&lt;a href=&quot;#Spider&quot; class=&quot;headerlink&quot; title=&quot;Spider&quot;&gt;&lt;/a&gt;Spider&lt;/h2&gt;&lt;p&gt;Spider默认需要填写三个参数：&lt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>scrapy专利爬虫（一）——scrapy简单介绍</title>
    <link href="http://blog.willshuhua.me/2018/01/16/scrapy%E4%B8%93%E5%88%A9%E7%88%AC%E8%99%AB%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94scrapy%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/"/>
    <id>http://blog.willshuhua.me/2018/01/16/scrapy专利爬虫（一）——scrapy简单介绍/</id>
    <published>2018-01-16T08:30:26.026Z</published>
    <updated>2018-01-16T08:30:48.221Z</updated>
    
    <content type="html"><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>scrapy是一款方便，快捷的开源爬虫框架。</p><blockquote><p>An open source and collaborative framework for extracting the data you need from websites.</p><p>In a fast, simple, yet extensible way.</p></blockquote><p>在<a href="http://blog.csdn.net/will4906/article/details/68955619" target="_blank" rel="noopener">上一版本中</a>，笔者采用selenium的方式进行数据采集，采集速度偏慢，而且有莫名的原因会导致第一次采集失败。改用scrapy之后，就像鸟枪换大炮一般，效果显著。</p><h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><ul><li>多线程</li></ul><p>尽管python中存在着GIL锁，导致多线程的效果不是特别理想，但是对于网络请求这种本身就需要等待的事件来说，多线程的作用还是非常大的。无需使程序花大量的时间在等待请求反馈上，可以腾出手去处理别的事情。</p><ul><li>默认自动去掉重复链接</li></ul><p>不停地访问一个网站对服务器的压力也是蛮大的，scrapy使用<a href="http://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/settings.html#dupefilter-class" target="_blank" rel="noopener">DUPEFILTER_CLASS</a>自动去除重复发送的请求。减轻了爬取对象服务器的压力，也降低了爬虫被发现的风险。</p><ul><li>简单易用，结构清晰</li></ul><p>借用一下官方的图</p><p><img src="http://img.blog.csdn.net/20170521010805701?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2lsbDQ5MDY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="scrapy架构图"></p><p>​    从图上可以看出scrapy分为,item,pipeline,scrapy engine,downloader,spider等几个部分。本项目只使用了一下 item, pipeline,downloader middlewares, spider等一部分组件。对于普通项目，使用这些部分也已经可以满足大部分需求。</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>笔者只在windows系统中尝试过scrapy，至于其他系统，请自行到查询。</p><blockquote><p>pip install scrapy</p></blockquote><p>使用pip安装即可，但是安装过程中经常会出现各种报错，通常都是以为安装过程中一些库安装不上所致。需要开发者查看安装过程中输出的报错，根据报错再到对应库的官网上将whl文件下载下来，用pip install 将whl文件安装即可。笔者在安装过程中遇到的问题是twisted的库安装不上，下载下来安装后便可正常。</p><h2 id="源码下载"><a href="#源码下载" class="headerlink" title="源码下载"></a>源码下载</h2><ul><li><a href="https://github.com/will4906/PatentCrawler" target="_blank" rel="noopener">github</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h3&gt;&lt;p&gt;scrapy是一款方便，快捷的开源爬虫框架。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;An open source and collabor
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>scrapy专利爬虫（四）——数据处理</title>
    <link href="http://blog.willshuhua.me/2018/01/16/scrapy%E4%B8%93%E5%88%A9%E7%88%AC%E8%99%AB%EF%BC%88%E5%9B%9B%EF%BC%89%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"/>
    <id>http://blog.willshuhua.me/2018/01/16/scrapy专利爬虫（四）——数据处理/</id>
    <published>2018-01-16T08:30:26.016Z</published>
    <updated>2018-01-16T08:32:03.845Z</updated>
    
    <content type="html"><![CDATA[<p>说到scrapy的数据处理，就必须先介绍两个组件item和pipeline。</p><h2 id="item"><a href="#item" class="headerlink" title="item"></a>item</h2><p>item的使用比较简单，只需要定义一个继承自scrapy.Item的类，在类中定义需要采集的元素即可，比如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 专利名称</span></span><br><span class="line">name = scrapy.Field()</span><br><span class="line"><span class="comment"># 类型（公告，授权公告）</span></span><br><span class="line">type = scrapy.Field()</span><br><span class="line"><span class="comment"># 专利类型</span></span><br><span class="line">patentType = scrapy.Field()</span><br><span class="line"><span class="comment"># 申请号</span></span><br><span class="line">requestNumber = scrapy.Field()</span><br><span class="line"><span class="comment"># 申请日</span></span><br><span class="line">requestDate = scrapy.Field()</span><br><span class="line"><span class="comment"># 公布日</span></span><br><span class="line">publishDate = scrapy.Field()</span><br></pre></td></tr></table></figure><p>在后续的使用中，可以直接使用<code>item.get(&#39;name&#39;)</code>取值，<code>item[&#39;name&#39;] = 数据</code>赋值。</p><h2 id="pipeline"><a href="#pipeline" class="headerlink" title="pipeline"></a>pipeline</h2><p>pipeline作为数据查重，收集等功能的管道，基本的数据处理都将这里进行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">process_item(self, item, spider)</span><br></pre></td></tr></table></figure><p>函数中的item是在之前spider中的回调函数yield的item，进入了这个函数后我们可以进行查重、校验和数据存储。本工程对发明人和申请人进行了校验。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> self.checkForInventor(item):</span><br><span class="line">        <span class="keyword">if</span> self.checkForProposer(item):</span><br><span class="line">            print(item.items())</span><br><span class="line">            self.writeToExcel(item)</span><br><span class="line">    <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><h2 id="写入excel"><a href="#写入excel" class="headerlink" title="写入excel"></a>写入excel</h2><p>由于项目对excel的需求不高，所以工程使用了xlrd，xlwd，xlutils等库，写入xls文件。</p><p>python读写excel是分别使用了两个库进行操作。写入用的是xlwd，读取用的是xlrd。而且没有提供直接修改的API，所以需要使用xlutils的copy先将excel文件读入写入实例的内存。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getExcel</span><span class="params">(self, mode)</span>:</span>           <span class="comment"># 采用工厂模式建立excel的读写实例</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(mode, str):</span><br><span class="line">        <span class="keyword">if</span> mode.upper() == <span class="string">"READ"</span>:</span><br><span class="line">            <span class="keyword">return</span> xlrd.open_workbook(self.__fileName)</span><br><span class="line">        <span class="keyword">elif</span> mode.upper() == <span class="string">"WRITE"</span>:</span><br><span class="line">            rb = xlrd.open_workbook(self.__fileName)</span><br><span class="line">            <span class="keyword">return</span> copy(rb)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br></pre></td></tr></table></figure><p>写入时需要先指定sheet然后再进行写入操作。由于各种奇奇怪怪的限制，所以工程模仿安卓的SharedPreferences的使用方式，先获取editor然后每次写入需要commit。</p><p>详细实例在代码中。</p><h2 id="源码下载"><a href="#源码下载" class="headerlink" title="源码下载"></a>源码下载</h2><ul><li><a href="https://github.com/will4906/PatentCrawler" target="_blank" rel="noopener">github</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;说到scrapy的数据处理，就必须先介绍两个组件item和pipeline。&lt;/p&gt;
&lt;h2 id=&quot;item&quot;&gt;&lt;a href=&quot;#item&quot; class=&quot;headerlink&quot; title=&quot;item&quot;&gt;&lt;/a&gt;item&lt;/h2&gt;&lt;p&gt;item的使用比较简单，只需要定
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>scrapy专利爬虫（三）——简单实际操作</title>
    <link href="http://blog.willshuhua.me/2018/01/16/scrapy%E4%B8%93%E5%88%A9%E7%88%AC%E8%99%AB%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94%E7%AE%80%E5%8D%95%E5%AE%9E%E9%99%85%E6%93%8D%E4%BD%9C/"/>
    <id>http://blog.willshuhua.me/2018/01/16/scrapy专利爬虫（三）——简单实际操作/</id>
    <published>2018-01-16T08:30:25.992Z</published>
    <updated>2018-01-16T08:31:15.040Z</updated>
    
    <content type="html"><![CDATA[<h2 id="确定链接"><a href="#确定链接" class="headerlink" title="确定链接"></a>确定链接</h2><p>在chrome中打开审查元素中的network选项，查看查询专利时发送的请求。观察后发现在每次查询的时候，浏览器都会先发送两条请求给服务器。</p><p><img src="http://img.blog.csdn.net/20170521010929121?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2lsbDQ5MDY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p><h2 id="发送相关请求"><a href="#发送相关请求" class="headerlink" title="发送相关请求"></a>发送相关请求</h2><p>经过观察发现，网站的查询流程是</p><ul><li>先发送不带参数的post请求preExecuteSearch!preExcuteSearch.do将ip地址传给服务器</li><li>然后再发送biaogejsAC!executeCommandSearchUnLogin.do将查询参数发给服务器</li></ul><h2 id="填写表单，发送请求"><a href="#填写表单，发送请求" class="headerlink" title="填写表单，发送请求"></a>填写表单，发送请求</h2><p>这里只给出一个简单的例子，具体实现见github或代码附件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">"Content-Type"</span>: <span class="string">"application/x-www-form-urlencoded"</span></span><br><span class="line">&#125;</span><br><span class="line">searchExp = SearchService.getCnSearchExp(self.startDate, proposer, inventor, type)</span><br><span class="line">formData = &#123;</span><br><span class="line">    <span class="string">"searchCondition.searchExp"</span>: searchExp,</span><br><span class="line">    <span class="string">"searchCondition.dbId"</span>: <span class="string">"VDB"</span>,</span><br><span class="line">    <span class="string">"searchCondition.searchType"</span>: <span class="string">"Sino_foreign"</span>,</span><br><span class="line">    <span class="string">"searchCondition.power"</span>: <span class="string">"false"</span>,</span><br><span class="line">    <span class="string">"wee.bizlog.modulelevel"</span>: <span class="string">"0200201"</span>,</span><br><span class="line">    <span class="string">"resultPagination.limit"</span>: BaseConfig.CRAWLER_SPEED</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">yield</span> FormRequest(</span><br><span class="line">    url=<span class="string">"http://www.pss-system.gov.cn/sipopublicsearch/patentsearch/biaogejsAC!executeCommandSearchUnLogin.do"</span>,</span><br><span class="line">    callback=self.parsePatentList,</span><br><span class="line">    method=<span class="string">"POST"</span>,</span><br><span class="line">    headers=headers,</span><br><span class="line">    formdata=formData,</span><br><span class="line">    meta=&#123;</span><br><span class="line">        <span class="string">'searchExp'</span>: searchExp,</span><br><span class="line">        <span class="string">'inventionType'</span>: type,</span><br><span class="line">        <span class="string">'startDate'</span>: self.startDate,</span><br><span class="line">        <span class="string">'proposer'</span>: proposer,</span><br><span class="line">        <span class="string">'inventor'</span>: inventor</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="数据解析"><a href="#数据解析" class="headerlink" title="数据解析"></a>数据解析</h2><p>通过观察chrome的Element，可以逐个找出我们所需要的元素，例如：</p><p><img src="http://img.blog.csdn.net/20170521011044233?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2lsbDQ5MDY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p><p>本工程使用beautifulsoup进行解析，对于带class的元素，使用<code>find(attrs={&quot;class&quot;: &quot;className&quot;})</code>的方法采集即可，其他参数也类似。这里提供简单的例子</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">itemSoup = BeautifulSoup(item.prettify(), <span class="string">"lxml"</span>)</span><br><span class="line">header = itemSoup.find(attrs=&#123;<span class="string">"class"</span>: <span class="string">"item-header"</span>&#125;)</span><br><span class="line">pi[<span class="string">'name'</span>] = header.find(<span class="string">"h1"</span>).get_text(strip=<span class="keyword">True</span>)</span><br><span class="line">pi[<span class="string">'type'</span>] = header.find(attrs=&#123;<span class="string">"class"</span>: <span class="string">"btn-group left clear"</span>&#125;).get_text(strip=<span class="keyword">True</span>)</span><br><span class="line">pi[<span class="string">'patentType'</span>] = QueryInfo.inventionTypeToString(type)</span><br><span class="line">content = itemSoup.find(attrs=&#123;<span class="string">"class"</span>: <span class="string">"item-content-body left"</span>&#125;)</span><br></pre></td></tr></table></figure><h2 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a>数据收集</h2><p>同样的需要对item使用yield，然后将数据传入pipeline中进行处理，关于更多数据处理的详细内容将会在下节内容中介绍。</p><h2 id="源码下载"><a href="#源码下载" class="headerlink" title="源码下载"></a>源码下载</h2><ul><li><a href="https://github.com/will4906/PatentCrawler" target="_blank" rel="noopener">github</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;确定链接&quot;&gt;&lt;a href=&quot;#确定链接&quot; class=&quot;headerlink&quot; title=&quot;确定链接&quot;&gt;&lt;/a&gt;确定链接&lt;/h2&gt;&lt;p&gt;在chrome中打开审查元素中的network选项，查看查询专利时发送的请求。观察后发现在每次查询的时候，浏览器都会先发送两
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>python序列化</title>
    <link href="http://blog.willshuhua.me/2018/01/16/%E5%BA%8F%E5%88%97%E5%8C%96/"/>
    <id>http://blog.willshuhua.me/2018/01/16/序列化/</id>
    <published>2018-01-16T08:29:39.199Z</published>
    <updated>2018-01-16T08:30:04.920Z</updated>
    
    <content type="html"><![CDATA[<h2 id="pickle"><a href="#pickle" class="headerlink" title="pickle"></a>pickle</h2><p>可利用pickle将python对象以二进制方式写进文件中。也可以利用pickle读出来。</p><ul><li>pickle.dump(obj, file, protocol=None, *, fix_imports=True)</li></ul><p>将python对象写进文件中。<br>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">filename = <span class="string">'model.pkl'</span></span><br><span class="line"><span class="keyword">with</span> open(filename, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    pickle.dump(instance, f)        <span class="comment"># instance是一个python对象</span></span><br></pre></td></tr></table></figure><ul><li>pickle.load(file, *, fix_imports=True, encoding=”ASCII”, errors=”strict”)</li></ul><p>将python对象从文件中读出。<br>e.g.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">filename = <span class="string">'model.pkl'</span></span><br><span class="line"><span class="keyword">with</span> open(filename, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    instance = pickle.load(f)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;pickle&quot;&gt;&lt;a href=&quot;#pickle&quot; class=&quot;headerlink&quot; title=&quot;pickle&quot;&gt;&lt;/a&gt;pickle&lt;/h2&gt;&lt;p&gt;可利用pickle将python对象以二进制方式写进文件中。也可以利用pickle读出来。&lt;/p&gt;
&lt;ul&gt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>数据库触发器</title>
    <link href="http://blog.willshuhua.me/2018/01/16/%E5%87%A0%E4%B8%AAsql%E8%A7%A6%E5%8F%91%E5%99%A8%E6%A0%B7%E4%BE%8B/"/>
    <id>http://blog.willshuhua.me/2018/01/16/几个sql触发器样例/</id>
    <published>2018-01-16T08:28:57.994Z</published>
    <updated>2018-01-16T08:29:26.708Z</updated>
    
    <content type="html"><![CDATA[<p>触发器虽然在sql标准中有支持，但是几个数据库仍然还是具有特殊的语法。笔者试着写了几个比较常用的触发器作为记录。</p><h2 id="DEMO"><a href="#DEMO" class="headerlink" title="DEMO"></a>DEMO</h2><p>使用一下两个表作为触发器的使用示例</p><ul><li>test1(<u>a</u>, b, c, d)</li><li>test2(<u>a</u>, c, d)<h3 id="Mysql"><a href="#Mysql" class="headerlink" title="Mysql"></a>Mysql</h3></li><li><p>insert</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">DROP TRIGGER IF EXISTS after_insert_trigger;</span><br><span class="line">DELIMITER $</span><br><span class="line">CREATE TRIGGER after_insert_trigger</span><br><span class="line">AFTER INSERT ON test1</span><br><span class="line">FOR EACH ROW</span><br><span class="line">BEGIN</span><br><span class="line">INSERT INTO test2 VALUES(NEW.a, NEW.c, NEW.d);</span><br><span class="line">END$</span><br><span class="line">DELIMITER ;</span><br></pre></td></tr></table></figure></li><li><p>update</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">DROP TRIGGER IF EXISTS after_update_trigger;</span><br><span class="line">DELIMITER $</span><br><span class="line">CREATE TRIGGER after_update_trigger</span><br><span class="line">AFTER UPDATE ON test1</span><br><span class="line">FOR EACH ROW</span><br><span class="line">BEGIN</span><br><span class="line">UPDATE test2 SET c = NEW.c, d = NEW.d WHERE test2.a = NEW.a;</span><br><span class="line">END$</span><br><span class="line">DELIMITER ;</span><br></pre></td></tr></table></figure></li><li><p>delete</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">DROP TRIGGER IF EXISTS after_delete_trigger;</span><br><span class="line">DELIMITER $</span><br><span class="line">CREATE TRIGGER after_delete_trigger</span><br><span class="line">AFTER DELETE ON test1 </span><br><span class="line">FOR EACH ROW </span><br><span class="line">BEGIN </span><br><span class="line">DELETE FROM test2 WHERE test2.a = OLD.a;</span><br><span class="line">END$</span><br><span class="line">DELIMITER ;</span><br></pre></td></tr></table></figure></li><li><p>这里AFTER 指在事件发生后执行，我们也可以使用BEFORE在事件发生之前执行触发器的动作。</p><blockquote><p>在事件之前被执行的触发器可以作为避免非法更新、插入或删除的额外约束。为了避免执行非法动作而产生错误，触发器可以采取措施来纠正问题，使更新、插入或删除合法化。[1]</p></blockquote></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">DROP TRIGGER IF EXISTS before_insert_trigger;</span><br><span class="line">DELIMITER $</span><br><span class="line">CREATE TRIGGER before_insert_trigger</span><br><span class="line">BEFORE INSERT ON test1</span><br><span class="line">FOR EACH ROW</span><br><span class="line">BEGIN</span><br><span class="line">  SET NEW.b = NEW.b + 1;</span><br><span class="line">END$</span><br><span class="line">DELIMITER ;</span><br></pre></td></tr></table></figure><ul><li>DELIMITER<br>DELIMITER是用来定义分隔符的，由于BEGIN和END之中的多行执行语句需要’ ; ‘作为结尾，导致命令行提前以为语句书写结束，导致报错，需要使用DELIMITER定义临时分隔符，如’$’保证语句的正常执行。<h3 id="Postgresql"><a href="#Postgresql" class="headerlink" title="Postgresql"></a>Postgresql</h3></li></ul><p>postgresql对触发器的语法是比较特殊的，需要先定义执行函数，然后在出发器中使用<code>EXECUTE PROCEDURE</code>来处理执行的行为。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">CREATE OR REPLACE FUNCTION after_insert_trigger()</span><br><span class="line">RETURNS TRIGGER AS $$</span><br><span class="line">BEGIN</span><br><span class="line">INSERT INTO test2 VALUES(NEW.a, NEW.c, NEW.d);</span><br><span class="line">RETURN NULL;</span><br><span class="line">END;</span><br><span class="line">$$</span><br><span class="line">LANGUAGE plpgsql;</span><br><span class="line">DROP TRIGGER IF EXISTS after_insert_trigger ON test1;</span><br><span class="line">CREATE TRIGGER after_insert_trigger</span><br><span class="line">    AFTER INSERT ON test1</span><br><span class="line">    FOR EACH ROW EXECUTE PROCEDURE after_insert_trigger();</span><br></pre></td></tr></table></figure><p>其他行为语法均与上面例子相似，笔者便不再赘述。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Abraham Silberschatz, Henry F.Korth S.Sudarshan. 数据库系统概念.杨冬青, 李红燕, 唐世渭等译. 北京. 机械工业出版社.2012.103-103.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;触发器虽然在sql标准中有支持，但是几个数据库仍然还是具有特殊的语法。笔者试着写了几个比较常用的触发器作为记录。&lt;/p&gt;
&lt;h2 id=&quot;DEMO&quot;&gt;&lt;a href=&quot;#DEMO&quot; class=&quot;headerlink&quot; title=&quot;DEMO&quot;&gt;&lt;/a&gt;DEMO&lt;/h2&gt;&lt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>首次测试</title>
    <link href="http://blog.willshuhua.me/2017/12/19/my-test/"/>
    <id>http://blog.willshuhua.me/2017/12/19/my-test/</id>
    <published>2017-12-18T17:40:51.387Z</published>
    <updated>2017-12-18T17:42:09.935Z</updated>
    
    <content type="html"><![CDATA[<h1 id="My-Test"><a href="#My-Test" class="headerlink" title="My Test"></a>My Test</h1><p>我只是试一试</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;My-Test&quot;&gt;&lt;a href=&quot;#My-Test&quot; class=&quot;headerlink&quot; title=&quot;My Test&quot;&gt;&lt;/a&gt;My Test&lt;/h1&gt;&lt;p&gt;我只是试一试&lt;/p&gt;

      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://blog.willshuhua.me/2017/12/19/hello-world/"/>
    <id>http://blog.willshuhua.me/2017/12/19/hello-world/</id>
    <published>2017-12-18T17:24:55.766Z</published>
    <updated>2017-12-18T17:24:55.766Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
